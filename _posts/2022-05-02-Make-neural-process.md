---
layout: post
title: "Neural process 만들기"
category: 만들기
---

# 서론

Neural process 비슷한걸 해보려다 망한게 있는데, 그걸 기반으로 한번 Neural process를 만들어보자.

컴퓨터의 리소스를 아끼기 위해 Colab에서 진행해보자.

# Representation Encoder

하나 생각해볼 만한 것이 있다.

함수는 n개의 점으로 이루어져 있고, 이를 어떠한 Representation 벡터로 나타내게 될 것이다.

그런데 어떻게 n개의 점 (길이는 가변)을 하나의 벡터로 나타낼 것인가?

한 방법은 하나의 Encoder를 만들어서, 각 점에 대해서 벡터를 만든 다음, 전부 평균내는 것이다.

뭔가 조금 주먹도끼로 내려치는 것 같은 그런 느낌을 주는 방법이긴 하지만 일단 돌아가기는 하는 모양이다.

# 차원 맞추기

Decoder로 넘어가보자.

이전에 한번 논했다시피 디코더에는 z와 x가 들어가는데, 이 둘을 단순하게 concat하여 신경망에다가 넣는다.

문제는 차원을 제대로 맞춰줘야 한다는 것이다.

z는 숫자 두개, x는 숫자 하나짜리가 여러 개 들어있는 형태가 될 것이다.

z가 [a, b], x가 [[x1], [x2], [x3]]이면

둘을 합친 것은 [[x1, a, b], [x2, a, b], [x3, a, b]]가 되게 해주는 것이 제일 자연스러울 것이다.

# ELBO

...라는 말은 꽤 자주 봤다. 아마도 로그 우도 + KLD를 ELBO라고 불렀던 것 같다.

VI(변수 추론)때 단골로 빠지지 않고 나오던 항이고, 사실상 Loss라고 보면 된다. (사실 마이너스 Loss지만)

그걸 여기에서 직접 구현을 해줘야 하는 모양이다.

로그 우도도, KLD도 가우스 분포에 맞춰서 직접.

뭐, [코드 보고 하는 거니까](https://chrisorm.github.io/NGP.html) 큰 부담은 없다.

동일 코드에는 context가 1개~4개가 되는데, 전체 데이터가 9개니까 대충 50%라고 치고 해보자.

# Sample a batch

뭔가 차원이 묘하게 샘플 코드하고 엇나가는가 싶더니.

원본 코드는 매 번 한 batch의 함수 '후보'들을 추출해서 학습하고 있었다.

그대로 코드를 고쳐주자.

# 뭔가 잘 안나온다.

으음... 이러면 깔끔하게 잘 나올꺼라고 생각했는데...

어째서인지 잘 안됐다. 정확히는 프로세스가 아니라 그냥 곧은 일자 함수가 나온다.

# 다음 시간

뭔가 엄청 당당하게 틀리고 있다는 느낌이다.

다음 시간에는 트러블슈팅을 해보자.

---
layout: post
title: "MNIST 7"
category: 만들기
---

# 서론

GAN이 이렇게 오래 걸릴지는 몰랐다.

이번에는 [이 링크](https://machinelearningmastery.com/practical-guide-to-gan-failure-modes/)를 참조해보기로 하자.

예전에 GAN을 처음 해볼 때 참고했던 사이트다.

Adam의 학습률과 beta값을 잘 조정해줘야 한다는 것 같다. (제일 마지막에 '이렇게 하면 이렇게 망합니다'의 예제로 Adam을 Generator와 Discriminator 둘 다 기본 설정대로 두고 한게 있다.)

너무 파인튜닝이 많이 들어가는 것 같지만 아무튼 해주자.

말 나온 김에 해보자면 GAN의 실패 케이스는 크게 두가지가 있다.

하나는 수렴하지 않는 것이다. 지금까지 내 코드가 대충 그런 느낌이었다. 뭔가가 껌딱지같은게 나온다거나 Discriminator가 전혀 학습을 하지 못해서 제대로 된 결과가 나오지 않는 것이다.

또 하나는 모드 붕괴다. 이건 무언가 수렴하기는 하는데, 그래서 이미지가 나오기는 하는데, 아무리 랜덤값을 넣어서 돌려봐도 같은 모양만이 나오고 다양성이 결여되는 현상이다.

아무튼 됐고, 코드를 조금 끄적끄적해서 나올 수 있는 (거의) 모든 이미지를 가져왔다.

애초에 latent space가 2차원으로 엄청 좁아서 다양성은 기대하지 않았다.

![success...maybe?](images/gan_maybe_success.png)

그래도 뭔가 글씨 비슷한걸 만드려 노력한 것은 보인다. 아주 칭찬해.

ConditionalGAN같은 다른 기술이 아니라 원하는 수를 보이게 하는건 불가능하다. (게다가 말했다시피 지금 latent space가 매우 매우 좁기도 하고.)

그래도 일단 기본 GAN은 숫자처럼 보이는 무언가가 나온다면 성공이다.

# Conditional GAN

말 나온 김에 Conditional GAN도 만들어보자.

참조용 소스 코드는 [이쪽](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/cgan/cgan.py)이다.

원리는 GAN하고 거의 같지만, Condition을 준다는게 차이다. Loss 함수에는 조건부 확률로 표현했고, 실제 코드로는 Embedding 레이어를 거친 class와 노이즈를 이어 붙여서 (concat) 신경망에 건네준다.

그 외에는 큰 차이가 없는 것 같다.

Latent는 1차원으로 줄여줘보자. (마지막에 한 장짜리로 첨부할 이미지를 만들 때 좋다. 최소한 '만들 수 있는 거의 모든 이미지'를 보여줄 수 있으니까.)

# 다음 시간

Conditional GAN이 제대로 작동하는걸 다음 시간에 볼 수 있지 않을까?

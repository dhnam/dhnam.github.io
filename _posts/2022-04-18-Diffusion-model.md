---
layout: post
title: "Diffusion model"
catogory: 공부하기
---

# 서론

일단 저번주 금요일에는... 확률 분포 표기하고 관해서 무언가 감이 와서 뻘짓하다가 시간을 다 날려먹었다.

대충 생각해보고 될 것 같은데? 싶어서 끄적거려봤는데 전혀 안돼버렸다. 

오늘은 Diffusion model에 대해서 알아볼 것이다. 전반적인 이미지에 대해서는 저번에 적었던 것으로 기억한다.

# 마르코프 체인

일단 아주 간단하게 마르코프 체인에 대해서 짚고 넘어가자. Diffusion Model에 대해서는 꽤 자주 보인다.

마르코프 체인은, 단순하게 말하면, 어떤 상태를 다른 상태로 바꾸는 확률 과정인데, 과거에 영향을 받지 않는다.

상태 전이에 과거의 영향을 전혀 받지 않고, 현재 상태에만 영향을 받는다면 이를 마르코프 연쇄라고 부른다.

예컨데 초등학교 중학교 문제로 자주 나오는 날씨 예를 생각해보자.

내일의 날씨가 오늘 비가 오는지 해가 맑은지에만 영향을 받는다면 이는 마르코프 연쇄이다.

내일의 날씨가 오늘, 어제, 그리고 엊그제... 등, 과거의 영향을 받는다면 이는 마르코프 연쇄가 아니다.

# Diffusion Process

Diffusion model은 두 부분으로 나뉜다. 노이즈를 넣는 부분, 그리고 노이즈를 제거하는 부분.

이 중 노이즈를 넣는 부분은 마르코프 연쇄인데, 학습하지 않는 부분이다.

그래서 할 이야기는 별로 없는 부분 같다.

아 참, 그리고 이건 다들 직감적으로 알고 있을테지만, 이미지에 작은 노이즈를 계속해서 더해가다 보면 언젠가는 완벽한 노이즈가 된다.

# Reverse Process

그 반대다.

이 한 마디면 사실 충분하다.

둘이 같은 것은 이 것도 Markov Chain이라는 것 정도다.

뭐, 말이 단순하게 '그 반대다', 지, 사실 조금 생각을 해 보면 어렵다.

거의 다 노이즈가 지워진, 살짝만 노이즈가 있는 이미지에는 단순한 노이즈 제거에 불구하다.

하지만 완전한 노이즈를 어떻게든지 멀쩡한 이미지로 바꿔내야 하는 것도 동일한 모델이다.

단순한 생각으로는 단순하게 불가능한 일이다.

이 부분이 학습되는 부분이다.

재미있는건 (원래 이 바닥이 그런건지) 노이즈 추가가 정규분포를 따랐듯. 노이즈 *제거* 역시 정규분포를 따른다는 것이다.

물론 노이즈 추가와 같은 평균/표준편차를 따른다면 노이즈가 제거될 리가 없다만...

아무튼 확률적으로 색깔을 입힌다고 생각하면 되겠다.

# 수식 전개...?

뭐 수식 전개는 대충 붙여놓고 뚫어져라 쳐다보는 것밖에 할 것이 없으니까 자세한 사항은 생략하고,

VAE와 똑같은 식에서 시작해서, 똑같은 과정을 거쳐가다가, 중간에 다른 방향으로 틀어버리는 모양이다.

그래서 최종적으로 나오는 식이 달라지는데, 이 식이 목적함수가 되고, Loss는 여기에서 나온다.

그래도 어떻게든 전개한 결과는 가져오는게 예의일 것 같다. 그게 있어야지 보면서 이야기할 거리도 있고.

...열심히 적고 있었는데 실수로 수식을 전부 날려먹은 관계로, 그냥 수식 전개 이미지를 가져오겠다.

중간 부분 무시하고 처음하고 끝을 보자.

![eq](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbykWBd%2Fbtq9fXAu3xj%2FHlt53RrxSmvKlW7ZZNdssk%2Fimg.png)

앞쪽 부분은 q(x\_T|x\_0)과 p\_theta(x\_0|x\_T)의 KL다이버전스다. 즉, 이 둘을 최대한 가깝게 만드는 것이 Loss의 목표가 된다.

이 둘은 각각 Diffusion 단계과 Reverse 단계라고 볼 수 있다.

또한 그 뒤에 Cross Entropy 항도 하나 붙어있다.

그 말고도 Term이 두개가 더 나온다... 아이고.

(Gaussian Process라는 말이 나오는데... 다음엔 여기에 대해서 공부해보자.)

# Loss

다른거 다 건너뛰고, 최종적인 Loss term을 살펴보자.

![loss](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FGZR7s%2Fbtq9e7w8o3b%2FnC6LRLt7VkpbkR500yFixK%2Fimg.png)

이미지에 표시되어 있듯, 세 부분으로 나뉜다.

1번 부분은 NN을 학습시키지 않는다. 분자 분모 어느 쪽이건 그냥 노이즈이기 때문이다. 뭔가 학습시킬 부분이 없다.

2번 부분이 중요한데- KL다이버전스들의 합이다. 여기서 분모는 첫 이미지와 t번 노이즈를 적용한 이미지가 있을 때 t-1번 노이즈를 적용한 이미지가 나올 확률이다. 즉 Ground Truth라고 보면 되겠다. 분자는 t번째 이미지가 있을 때, t-1번째 이미지가 나올 확률 분포다. 즉 이 쪽을 학습시킨다고 보면 되겠다.

3번 부분은 2번 부분에서 학습하지 못하는 마지막 한 칸을 학습한다.

이렇게 보면 직관적인데, 이걸 수학적으로 뒷받침해주는 전개 과정이 나한테는 너무 어렵다...

# 다음 시간

Gaussian Process에 대해서 알아보자.

---
layout: post
title: "Optimizer 만들기 3"
category: 공부하기
---

# 서론

저번에 만드려고 했지만 그 전에 보강해야 할 것이 너무 많아서 중간에 멈췄었다.

이번에야말로 돌아가는 무언가를 만들어보자.

# 문제 분석과 해결 방안 고찰

가장 큰 문제는 역시 학습이 되지 *않는다*는 점이다.

심플한 문제다.

가지고 조금 놀아본 결과 그래디언트 계산 자체는 잘 되는데, 업데이트가 (id라던가 이런 저런 문제로) 전혀 업데이트되지 않고 있는 모양이다.

해결을 위해서 이것 저것 확인하다가 np.copyto()라는 메서드를 알게 되었다. 이걸 잘 활용하면 되지 않을까.

좋다. 이제 딱 한가지 문제만 해결하면 된다. 이제 Loss가 매 iteration마다 바뀐다.

남아있는 문제란 - Loss가 줄어드는 대신 늘어난다는 것이다. 어딘가 잘못한걸까.

일단 보기 좋으라고 학습하는 동안 어떻게 변하는지를 찍어보기로 했다. 코드를 가지고 조금 놀다 보니까 괜찮은 애니메이션이 나왔다. (GIF로 저장하고 어쩌고 하는건 조금 까다로우니까 이미지는 생략한다.)

표현력 부족인가도 싶어서 (가끔 있는 일이다) 레이어를 늘려줘봤지만 크게 나아지지는 않는 것 같다.

주목할만한 점은, 그래프가 계단 모양으로 생겼다는 것이다. 즉, 0 근처에서만 변화가 일어나고 나머지 부분은 거의 변화가 보이지 않는다는 것이다.

Sigmoid를 사용한게 잘못인가? 싶다. (Vanishing Gradient 문제)

이걸 확실히 알아보려면 ReLU를 구현해야 하는데... 조금... 까다로울 것 같다.

게다가 뭔가 느낌이 Sigmoid의 문제는 아닐 것 같기도 하다. 뭔가가 구현을 잘못했나?

하나 확실한건 sigmoid의 gradient가 계산이 잘 된다는 것이다. 막 WolframAlpha로 확인했다.

진짜 sigmoid가 문제인지는 pytorch를 이용해서도 확인해볼 수 있을 것 같다.

PyTorch를 이용해서 확인해본 결과, Sigmoid + SGD라는 조합 자체도 문제가 있지만, 내 구현에도 어딘가 문제가 있는 것 같다.

아무래도 어떤 문제가 있는지는 내일 마저 확인해보자.

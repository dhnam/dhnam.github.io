---
layout: post
title: "Autograd 만들기 6"
category: 만들기
---

# 서론

저번에 이어서 계속 만들자.

슬슬 거의 다 완성되어 가는 기분이지만, 곰곰히 생각해보니 아직 부족한 부분이 있다.

지금은 그래프가 아니라 사실상 트리이다. 즉, 한 노드의 gradient 값은 다른 노드 하나에만 의존한다.

(그래프의 노드에서 뻗어나가는 간선이 하나 뿐이다.)

간선 갯수가 더 많을 때에도 어떻게 잘 처리해야 할 것이다.

이걸 어떻게 처리해야 할지는 약간의 수학을 동원해야 할 것이다.

# 수학

인터넷을 찾아 보면 금방 나올 것 같긴 하지만, 일단 종이와 샤프로 시작하자.

먼저, 연쇄 법칙이라는 법칙이 어디서 나왔는지를 생각하자.

이전에도 적었듯 f(g(x))의 미분에서 나온 것이었다.

![eq1](https://latex.codecogs.com/svg.image?(f(g(x)))%27%20=%20f%27(g(x))g%27(x))
![eq2](https://latex.codecogs.com/svg.image?%5Cfrac%7Bdy%7D%7Bdx%7D%20=%20%5Cfrac%7Bdy%7D%7Bdz%7D%20%5Cfrac%7Bdz%7D%7Bdx%7D%5C;%5C;(%5Cfrac%7Bdy%7D%7Bdz%7D%20=%20f%27(g(x)),%20%5Cfrac%7Bdz%7D%7Bdx%7D%20=%20g%27(x)))

이걸 확장시키는거로 시작해보자.

지금 문제가 되는 부분은 이 부분인데...

![eq](https://latex.codecogs.com/svg.image?%5Cfrac%7B%5Cpartial%20%7D%7B%5Cpartial%20x%7Df(g(x),%20h(x))%20=%20???)

이건 어떻게 될까?

# 추측 - 커링?

이거에 대한 답을 뒤져보기 전에, 예전에 '커링'이라는 개념에 대해 들어본 적이 있다.

프로그래밍 언어 하스켈(Haskell)에서 꽤 자주 쓰이는 용어인데...

대충 인수를 여러개 가진 함수를 인수 한 개 가진 함수 여러개로 분리해서 연쇄 적용하는 그런걸 커링이라고 부른다.

인수가 한 개일 때는 이미 처리할 수 있으니까 (연쇄 법칙) 이걸 어떻게 어떻게 잘 활용하면 뭐라도 되지 않을까?

사실 잘 안될 것 같은게... 커링은 이런거다. (위키백과에서 발췌)

![eq](https://wikimedia.org/api/rest_v1/media/math/render/svg/c91fd909768594728f54bfd734a57bb4605c9e19)

여기서 h, i는 함수이다. 즉 다중 인수 함수에 인수를 하나만 적용한 함수를 만드는 것이다.

함수를 내놓는 함수... 라는게 조금 다루기가 까다롭다. 최소한 지금까지 배운 수학으로는 어떻게 처리가 안된다.

이 방법으로는 어떻게 머리가 굴러가지를 않는다.

# 인터넷을 찾아보자.

인터넷에 나온 답으로는 '그냥 전파되어 온 gradient를 전부 더해라'라고 하는데...

이걸 어떻게 정당화시킬 수 있는가?가 궁금하다.

찾아봤더니 왠지 뭔가 예전에 배웠지만 까먹었던 것 같은 그런 기분이 든다.

![eq](https://latex.codecogs.com/svg.image?%5Cfrac%7Bdz%7D%7Bdt%7D=%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D%5Cfrac%7Bdx%7D%7Bdt%7D&plus;%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20y%7D%5Cfrac%7Bdy%7D%7Bdt%7D)

분명 배웠던 것 같다. 이걸 완전히 까먹고 있었다 뿐이지.

이거에 대한 복습은 내일 하도록 하자. 오늘은 일단 구현.

어쨌든 전파돼온 Gradient를 전부 더하면 된다. 간단하다.

구현 상으로도 거의 바뀌는건 없다.

# 다음 시간

구현을 아주 조금 수정했다.

다음에는 위에 말한 것처럼 다변수 함수의 미분에 대해 복습해보도록 하자.

그래도 자동미분은 거의 다 구현되어가는 것 같다.

다음에 자동미분을 만들 땐 항목별 곱, 나눗셈, concat 등에 대한 backward를 구현해보자.

그래도 지금 당장 단순한 인공 신경망은 구현 가능할꺼다.

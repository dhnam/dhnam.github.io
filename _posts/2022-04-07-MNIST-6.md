---
layout: post
title: "MNIST 6"
category: 만들기
---

# 서론

Generator의 구조를 [이 코드](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py)에 나온데로 고쳐보자.

대충 구조는 LeackyReLU를 사용하고, CNN을 사용하지 않는 모습이다.

Normalize는... 귀찮으니까 빼겠다.

Discriminator 구조도 고쳐주겠다. 역시 CNN같은건 안쓰는 모양이다.

뭔가 조금씩 가운데에 덩어리가 생긴걸로 봐서 이번에야말로 무언가 되는 기분이다.

분명 초반에는 뭔가 되는 기분으로 글자 모양도 생겼는데 어느새 덩어리가 돼서 굳어버렸다.

![img](/images/ganfailed.png)

증거사진을 하나 첨부한다. 분명 초반에는 느낌이 좋았는데

![img2](/images/ganfailed2.png)

이런 식으로 망하기도 한다. 도대체 뭐가 문제지.

예전에 비슷하게 망했을 때의 문제는 데이터가 0~1로 정규화가 되지 않은 것이었는데, 이번에는 정규화가 제대로 된 것도 확인했다.

크게 차이를 만들지 않아야 할 것 같지만, Generator의 마지막 activation이 무엇인지에 따라 뭔가 차이가 크게 나고 있다.

Sigmoid일 때, ReLU나 LeakyReLU일 때, Tanh일 때가 전부 다른 방식으로 망한다. Sigmoid는 빠르게 가운데로 뭉치고, Tanh로 할 때는 빠르게 가생이로 뭉친다. 또 ReLU는 잘 되는 것 같다가 나중에 보면 망해있다.

Loss의 변화를 살펴보면, (예상했다시피) Sigmoid와 Tanh는 Generator loss가 빠르게 수십으로 치솟고, Discriminator loss는 빠르게 0으로 수렴한다.

ReLU계열일 때는 나름 서로 밸런스 게임을 하고 있는거로 보이기는 하는데...

원본 코드를 따라서 LeakyReLU의 값(0보다 낮을 때 곱해지는 계수)을 0.2로 줬더니(아마 기본값은 0.1일꺼다.) 이번엔 Discriminator가 고장난다. 뭐지.

이번에는 batch normalization을 넣었는데- 아무래도 이걸 빼기로 한게 실수 같다.

특히 batch norm의 eps값을 엄청 크게 주는데(기본값 0.00001, 주는 값 0.8) 아마 이게 생각보다 중요한 역할을 하지 않았을까.

근데 이놈 또 잘 가는 척하다가 급발진이다.

# 다음 시간

다음에는 이게 더 잘 됐으면 좋겠다.

뭐 어제보다는 결과가 좋아졌으니 내일이면 이것보다 더 결과가 좋아져주지 않을까

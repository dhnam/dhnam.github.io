---
layout: post
title: "Loss 만들기"
category: 만들기
---

# 서론

오늘은 loss를 클래스로 구현해보자.

신경망의 필수적인 구성 요소이지만 지금까지는 일단 함수로 때웠다.

사시이이이일.... 큰 차이는 없을 것 같다.

그래도 일단 뭐라도 해야겠으니까 이번엔 그 쪽을 건드려보자.

# 구조

구조도 별 볼 일은 없을 것 같다.

함수를 감싸는 클래스 정도?

클래스에 `__call__` 하나 넣고 final 해준 다음에 `apply` 같은 이름으로 실제 구현을 넣고 그 부분을 abstract로 해주면 될 것 같다.

# MSE

겸사겸사 공부도 겸하기로 했다. 구현할게 진짜 거의 없어서...

MSE는 Mean Squared Error의 줄임말이고, 가장 단순한 Loss이다.

말 그다로 제곱 평균인데, 각 항목을 뺀 값을 제곱하여 모든 항목에 평균을 낸다.

주로 regression 문제에 쓰인다.

# 교차 엔트로피

정보 이론과 관련 있는 loss이다. 일단 엔트로피가 무엇인지에 대해 알고 가는게 좋다.

## 정보량과 엔트로피

정보의 양은 기본적으로 '놀람의 정도'이다. 예를 들어보자.

'내일 아침에 해가 동쪽에서 뜬다'는 문장은 전혀 놀랍지 않다. 확률이 100%니까.

'동전을 던졌더니 뒷면이 나왔다'는 문장은... 뭐 최소한 윗 문장보다는 놀랍다. 확률이 50%니까.

'동전을 10개를 던졌더니 전부 뒷면이 나왔다'는 문장은 꽤 놀랍다. 1/1024라는 작은 확률을 가지고 있으니까.

'로또에 당첨됐다'는 문장은 엄청 놀랍다. 말 그대로 로또 당첨의 확률이 필요하니까.

'슈퍼마리오 64 스피드론 중에 우주선의 간섭으로 데이터가 바뀌어 순간이동되었다'는 문장은 믿을 수 없을 정도로 놀랍다. 말 그대로 천문학적인 확률이 필요하니까.

이 중 정보량이 많은 것은 마지막 문장이다. *절대로 문장이 길어서 정보량이 많은 것이 아니다.* 그 다음으로 정보량이 많은 것은 가장 문장이 짧은 로또 당첨이니까.

확률이 낮을 수록 정보량이 많아진다는건 단순하게 표현할 수 있다.

그냥 확률의 역수로 표시하면 된ㄷ... 잠깐, 이게 아니다. 그렇게 간단하게는 되지 않는 모양이다.

왜일까 확률이 1일때의 정보양을 0으로 맞춰주고 싶어서일까 거기에 log를 취한다.

사실 이유가 하나 더 있다. 두 사건이 일어날 확률을 구할 때는 두 확률을 곱해준다.

정보량은 직관적으로 생각했을 때 두 사건의 정보량을 더해서 새로운 정보량이 되면 좋을 것 같다.

그런 조건을 만족하는 함수가 로그다.

아무튼 그래서 정보는 -log(확률)이다. (log(1/확률)과 -log(1/확률)은 같은 함수이다.)

## 엔트로피

정보에 대해서는 알아봤다. 엔트로피란 무엇일까?

간단하게 정리하면, 정보량의 기댓값이다.

예컨데 동전을 던진다고 하자. 이 '동전 던지기'에서 기대할 수 있는 정보량은 어느 정도일까?

(참고로 여기서 로그의 밑은 2를 쓴다.)

동전을 던져서 앞면이 나올 확률: 1/2

앞면이 나왔을 때의 정보량: 1

뒷면이 나올 확률: 1/2

뒷면이 나왔을 때의 정보량: 1

기댓값이라고 하면 평균을 내는 것임을 생각해보면... 1/2 * 1 + 1/2 * 1이 되어 전체 정보량의 기댓값은 1이 될 것이다.

이 1이 엔트로피가 된다.

조금 다른 예를 들어볼까? '동전을 두 개 던졌을 때 나오는 앞면의 수'의 엔트로피는 어떨까?

앞면이 0개일 확률: 1/4

정보량 : 2

앞면이 1개일 확률: 1/2

정보량 : 1

앞면이 2개일 확률 : 1/4

정보량 : 2

전체 평균 (엔트로피): 1/4 * 2 + 1/2 * 1 + 1/4 * 2 = 3/2 (1.5)

엔트로피의 한가지 특징은, 확률 분포가 고를수록 엔트로피는 커지고, 확률 분포가 불규칙적일 수록 엔트로피는 작아진다는 것이다.

## 교차 엔트로피

엔트로피를 살짝 더 복잡하게 생각하면, 그 확률 분포가 가지고 있는 정보량의 평균이다.

교차 엔트로피는 두 확률 분포가 얼마나 비슷하게 생겼는지를 계산한다.

이렇게 생각해보자. 확률이 낮은 항목은 정보량이 크다. 엔트로피를 계산할 때엔 각 항목에 대해 확률 * 정보량을 하는데...

만약 '확률'과 '정보량'이 다른 확률 분포에서 나왔다면?

'확률'이 나온 확률 분포를 A라고 하고, '정보량'이 나온 확률 분포를 B라고 하자.

A가 B와 같을 때 나오는 '엔트로피' 값은 가장 작다. 하지만 A와 B가 달라질수록 그 값은 커진다.

확률 분포가 '얼마나' 다른지를 측정할 수 있는 방법을 찾은 것이다. 이를 '교차 엔트로피'라고 부른다.

# 다시 구현으로 돌아와서

교차 엔트로피 구현은 조금 까다로웠는데... 뭐, 까다로*웠*다 뿐이지 이미 구현해둔 것이다.

# 다음 시간에

가장 중요한 두 가지 Loss들에 대해서는 알아봤다.

주로 MSE는 Regression에, Cross Entropy는 Classify에 쓰이는데, 진짜 자주 쓰인다. (softmax라던가 하는 것과 함께)

다음 시간에는 다른 Loss들에 대해서도 알아보자.

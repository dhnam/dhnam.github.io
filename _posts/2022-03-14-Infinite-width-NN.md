---
layout: post
title: "Understanding Infinite-width Deep Neural Networks"
category: 공부
---

# 서론

포항공대에 다니는 친구 녀석한테 포공 머신러닝 세미나 영상들을 받았다.

코딩 하던 것도 뭔가 더 만들 느낌이 안나고 해서 그 중에 하나의 영상을 보기로 했다.

이번 제목은 '무한 너비 심층인공신경망 이해하기' (Understanding Infinite-width Deep Neural Networks)다.

아마...도 수학적인 내용이 나올 것 같다.

# 왜 딥 러닝이 돌아가는가?

일단 딥러닝은 돌아간다. 이건 부정할 여지가 없는데... 이 이론적 바탕은 글쎄다.

왜 딥러닝이 돌아가는지에 대해서는 아직 제대로 알려지지 않았다.

각종 학습에 도움을 주는 기술들이 있기는 하지만 이들이 어째서 도움이 되는지 역시 알 수 없다.

게다가 예쁘게 오목한 표면 위에서 하는 학습도 아니고.

어째서 돌아가는지를 알기 위해서는 상황을 조금 단순화시킬 필요가 있다.

그리고 거시적인 관점에서 보는 것은 상황을 단순화시키는데 도움이 된다.

# 거시적 NN

일단 Width가 넓어지면 넓어질수록 출력되는 함수의 모양이 단순해진다.

또한 NN을 크게 네 영역으로 나눌 수 있어지는데, 이는 학습 방법 (베이지언 학습 / 경사 하강)과 관심을 가질 공간 (매개변수 공간 / 함수 공간)으로 나뉜다.

|\|매개변수 공간|함수 공간|
|---|---|---|
|베이지언 학습|베이지언 신경망|신경망 GP(NNGP)|
|경사 하강|선형화 신경망|뉴럴 탄젠트 커널|

# 베이지언

주로 학습 비용이 높아서 잘 쓰지 않는 방법이다.

무한히 큰 NN에는 효과적인 방법으로 학습할 수 있다고 한다.

(GP: 가우시안 프로세스와 관련이 있다.)

간단하게 정리하면 가우시안 프로세스로 NN을 단순화시켜 보일 수 있다는 내용이다.

# 경사 하강

조금 더 현실적으로 자주 쓰이는 방법이다.

Width가 무한히 늘어나면 NTK라는 객체로 생각이 가능해지는 모양이다.

또한 너비가 넓어질 수록 GD하는 동안 파라미터의 변화가 적어진다.

너비가 무한일 때 선형 근사가 가능해진다. (매개변수에 대해 선형적)

Architecture search에 사용할 수 있다고 한다.

# 총평

분명 한국어로 설명하고 있는데 이해를 못하겠다;

아무래도 큰 그림은 대충 잡은 모양이기는 하지만 그 외에는 진짜 잘 모르겠다.

그래도 대충 이런 분야가 있다는 것을 알아갈 수 있다는 것을 의의로 생각하자.
